La diffusione capillare dei Big Data nell'esistenza quotidiana ha generato considerevoli mutamenti, non soltanto nel settore dell'Informatica, ma ha anche lanciato nuove sfide per risolvere il dilemma tecnologico attuale: elaborare immediatamente l'intero flusso di dati.
Inizialmente, nell'analisi introduttiva, è stato evidenziato come le metodologie di riferimento per le applicazioni enterprise e successivamente le tecnologie supportate dai Big Data abbiano manifestato delle lacune quando è emerso il ruolo sempre più cruciale dell'accelerazione con cui i dati vengono processati.
Questo ha spinto verso l'adozione di approcci innovativi come le tecniche di \textit{streaming} nel contesto del processing dei dati in tempo reale, un'alternativa alle tradizionali metodologie basate su \textit{MapReduce}.
L'attenzione si è concentrata sullo stato attuale delle architetture che incorporano queste nuove metodologie.

Sotto l'aspetto progettuale e applicativo, le comunità \textit{open source} hanno risposto prontamente sviluppando una gamma completa di framework per agevolare l'elaborazione su larga scala dei dati. Da \textit{Hadoop} e \textit{Spark} per il calcolo parallelo distribuito a \textit{Kafka} per la gestione dei messaggi, passando per \textit{HDFS} e le numerose implementazioni di database \textit{NoSQL} per lo storage distribuito.
In questo panorama estremamente diversificato, il presente elaborato ha cercato di elevare il toolkit \textit{Akka} a un ruolo di "orchestratore" dei processi, abbracciando i principi fondamentali del suo modello basato su attori.
Attraverso la sua semplicità ma al contempo efficienza, il modello ad attori affronta efficacemente alcune delle sfide tipiche di un sistema software concorrente, in cui le risorse sono contese dagli elementi operanti internamente.
Sfruttando poche regole di base fondate sulla natura asincrona dei messaggi, sull'immunità al cambiamento e sulla mancanza di condivisione dello stato, Akka si presenta come uno strumento valido per i programmatori che gestiscono attori diversi tra loro, ciascuno con compiti e funzioni specifiche ma con la necessità di comunicare e scambiarsi informazioni in tempo reale.

Seguendo questa linea guida, nel corso di questa tesi è stata concepita e implementata la piattaforma, mirata a consentire la cooperazione di attori con ruoli differenti.
L'obiettivo della piattaforma è fornire analisi predittive e statistiche su grandi volumi di dati, facendo affidamento sui framework di mercato per il trattamento dei Big Data e sull'apprendimento automatico precedentemente menzionati: Spark, HDFS e Kafka.
Il software fornisce al programmatore un livello di astrazione che lo libera dalle complessità dell'interazione con i vari framework adottati, delegando all'astrazione il compito di comunicare con essi per implementare una pipeline applicativa che estrae il dato nella sua forma grezza direttamente dalla fonte, lo trasmette in modo asincrono alle componenti incaricate di analizzarlo e infine lo indirizza verso le funzioni di previsione e analisi statistiche.
Queste funzioni costituiscono il cuore pulsante della piattaforma, consentendo la costruzione iterativa di modelli di apprendimento automatico che, ad ogni iterazione, si ricostruiscono in base all'unione di un dataset storico e uno in tempo reale in continuo arricchimento.
Questo dimostra una notevole adattabilità nella selezione del modello più accurato per i dati soggetti a mutazioni periodiche.

Una parte significativa dell'output prodotto dalla piattaforma è rappresentata dai file generati periodicamente dalla piattaforma, da cui sono stati derivati grafici analoghi a quelli tradizionalmente utilizzati nell'ambito dell'analisi avanzata, dimostrando che un approccio simile fornisce anche gli strumenti necessari per eseguire analisi più approfondite sui dati.

Il progetto, tuttavia, rappresenta ancora un vasto terreno fertile per potenziali sviluppi ed estensioni.
Rispetto al caso di studio trattato, sarebbe un valore aggiunto esaminare una situazione che produca dati con un \textit{throughput} estremamente elevato, al fine di valutare le potenzialità di scalabilità della piattaforma in termini di streaming dei dati.
Attualmente, la piattaforma manca di un'interfaccia grafica, e i dati di output vengono acquisiti direttamente dai file prodotti durante l'elaborazione.
Sarebbe pertanto funzionale sviluppare un'applicazione web con moderne tecniche reattive che funga da dashboard per monitorare l'operato degli attori e i risultati prodotti.
Gli algoritmi di apprendimento automatico adottati sono stati selezionati tra quelli messi a disposizione da \textit{Spark} e configurati al minimo necessario per ottenere valori di accuratezza apprezzabili nei modelli del caso di studio affrontato.
Tuttavia, è necessario perfezionare il tuning dei parametri e valutare, contestualmente alle esigenze di nuovi casi di studio, l'opportunità di introdurre nuovi algoritmi che sfruttino non solo le tecniche di regressione e classificazione finora utilizzate.

In conclusione, l'elemento di novità principale proposto in questa tesi, ossia il modello basato su attori per definire un livello di astrazione applicativa che coordini attività di analisi avanzata sui dati, potrebbe aprire la strada a nuove prospettive in un contesto architetturale in costante evoluzione, che deve adeguarsi costantemente alla rivoluzione digitale in corso.
Tale rivoluzione è alimentata non solo dalla nascita di nuovi dispositivi e servizi, ma anche dal processo inesauribile di ricerca del perfezionamento.